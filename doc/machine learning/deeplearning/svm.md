# 监督分类算法之支持向量机（Support Vector Machine）

介绍： 在机器学习中，SVM是一种通过学习算法分析数据来达到二元分类或者回归的模型算法，可以通过ovo、ovr或者ova策略进行多分类，分类过程是对学习样本的最大边距超平面进行求解的过程，超平面由少数支持向量决定，解决了以往需要无穷大样本数量的问题，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了维数灾难。该算法在被广泛应用于模式识别中。

## 优点



- 专门针对有限样本情况，其目标是得到现有信息下的最优解而不仅仅是样本数量趋于无穷大时的最优值；
- 算法最终转化为一个二次型寻优问题，理论上得到的是全局最优点，解决了在神经网络方法中无法避免的局部极值问题；
- 支持向量机算法能同时适用于稠密特征矢量与稀疏特征矢量两种情况，而其他一些文本分类算法不能同时满足两种情况。
- 支持向量机算法能够找出包含重要分类信息的支持向量，是强有力的增量学习和主动学习工具，在文本分类中具有很大的应用潜力。



## 应用

- 人脸识别：划分图片中脸的边界
- 文本/超文本分类：通过感应式或者传导式模型分类，分类时给出概率值，然后与阈值比较
- 图片分类：与传统基于检索查询的方式相比，svm能提供更高的准确度
- 生物信息识别： 蛋白、基因分类..

## 线性分类

若输入数据所在的特种空间中存在决策边界（decision boundary）的超平面将学习目标按正类和负类分开，并使任意样本点到超平面的距离大于1，则称该分类问题具备线性可分性。

满足该条件的决策边界实际上构造了两个平行的超平面作为间隔边界以判别样本的分类

wX +b >= +1

wX + b <= -1

所有在间隔边界上方的认定为正类，间隔边界下方的认定为负类，两个间隔边界的距离d= 2 / |w|代表边距（margin），间隔边界上的正负样本为支持向量。这样的间隔我们称之为**硬间隔**。

## 损失函数

在一个分类问题不具备线性可分时，引入损失函数来量化以超平面作为分类边界带来的损失，即部分支持向量不落在超平面上，而是在间隔边界内部或者另一侧,这样的间隔我们称之为**软间隔**

![](./svm_lossfunc.jpg)

在正负样本点上引入负对数函数最小化损失，企图找到一组让损失最小的参数，在损失函数的最后时L2范数正则化（权值的平方，然后开根），通过惩罚数值，降低模型的复杂度来防止过拟合，其对离群值的容忍度较L1范数低。

## 非线性分类

一些线性不可分的问题，可能在维度更高的空间线性可分，即使用非线性函数（核函数）将数据从原始的特征空间映射到更高维度的希尔伯特空间，以寻求一个超平面将正负样本分开。

**核函数**

任意两个样本点在扩维后的空间的内积，如果等于这两个样本点在原来空间经过一核函数的输出，那么该函数就称为核函数。核函数必须满足**Mercer定理**(任何半正定函数都可以作为核函数)。

核函数包括线性核函数、多项式核函数、高斯核函数等，其中高斯核函数最常用，可以将数据映射到无穷维，也叫径向基（rbf）核函数

## 训练方法

得到最终的SVM分类器的算法包括次梯度下降和坐标下降，这两种方法在处理大的稀疏数据时，被验证有着显著的优点。当存在大量的训练样本时，次梯度法特别有效；当特征空间的维度特别高时，坐标下降特别有效。

**次梯度下降**

采用随机梯度下降（SGD）方法，只是不在函数梯度的方向上前进，而是从函数的次梯度中选出的向量的方向上前进，该方法的优点迭代次数不随训练样本数量的增加而增加或者减少

**坐标下降**

SVM的坐标下降算法基于对偶问题，对所有样本进行迭代，使系数Ci与函数关于C的偏导一致。然后，将所得的系数向量  投影到满足给定约束的最接近的系数向量（通常使用欧式空间）。然后重复该过程，直到获得接近最佳的系数向量。所得的算法在实践中运行非常快。





