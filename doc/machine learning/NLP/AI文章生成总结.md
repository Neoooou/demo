# AI文章生成总结

目前AI文章生成普遍使用循环神经网络来学习文本中的序列信息、理解语义，根据种子文本来预测下一个词，从而达到文本生成的目的。以循环神经元为基本神经单元来设计神经网络结构，
常见技术手段有：
	1）LSTM/GRU生成模型
	2）encoder-decoder + attention
	3）GAN （生成对抗网络）
基于字的生成效果较差，使用分词工具将语料库分词之后，生成出来的文本效果较好

## 循环神经网络（RNN - Recurrent Neural Network）
	神经网络的一种，用于处理序列化的输入。将输入序列中位置为t的token看作时刻t的输入，t时刻的输出不仅受到t时刻的输入影响，也受t-1时刻的输出影响。
	但是由于长期的递归会导致权重出现爆炸（gradient explosion）或消失（gradient descent）的问题，难以捕捉长期实践关联，但是LSTM和GRU的出现使这一问题
得到解决，LSTM和GRU属于RNN的子类

	### LSTM
	LSTM神经细胞传递两个状态值给下一神经元：c_state（细胞状态）和h_state（隐藏状态）,通过这两个状态值的增减来控制序列信息的传递。LSTM神经元内部信息的处理分为三个门：忘记门、输入门、输出门，
	忘记门：决定c_state需要丢弃多少信息
	输入门：决定将新的信息选择性地记录到细胞状态中
	输出门：基于细胞状态决定我们该输出什么
	
	### GRU
	LSTM的一个变体，将忘记门和输入门合成了一个单一的更新门，同时还混合了c_state和h_state为一个单一的时态值，达到的效果与LSTM类似，但是由于其模型的简单性，其需要的计算能力没有LSTM那么高

## 三方工具库：
	keras、jieba、pkuseg

## 1）LSTM/GRU生成模型
	以LSTM或者GRU为核心神经细胞的生成模型
	模型结构：Word Embedding + LSTM/GRU(单层或多层) + Dense（单层或多层）
	训练数据：以长度N的序列为输入，N+1时刻的词为输出，从头至尾如此划分语料库，得到训练的输入和输出集合



## 2）encoder-decoder + attention
	训练数据：输入序列和输出序列分别对应语料库的上句和下句，对输入和输出序列进行裁剪至固定长度
	编码器（encoder）和解码器（decoder）分别对应着输入序列和输出序列	的两个循环神经网络，通常分别在输出序列和输入序列头尾加上<go>，<eos>表示序列的开始和结束
	假设编码器输入x1,x2,...,xt经过变换后变成变量(h1,c1),(h2,c2),...,(ht,ct)，解码器以(ht,ct)为初始化状态值获取编码器的内容，进行变换后得到y1,y2,yt
	输出序列的每一个时刻获得的信息一定只能通过状态值获得，但是假设我们希望输入序列第一时刻的c只对应输出序列对应的第一时刻和第二时刻，而输入序列的第二时刻只对应输出序列的第三时刻。我们希望输入每一个部分分布的注意力是不一样的，这就是注意力机制。我们假设ct'是c背景向量的不同时刻不同的c，每一时刻的c对编码器中所对应的每个隐藏层进行一个加权平均，这个加权平均的权重使我们学习的参数，我们分配对应时刻隐藏h的权重的大小就是输入时刻每一时刻的注意力的大小。
	文本生成思路：使用以上方法进行编码解码，解码器的初始化token使用<go>,当解码出来的序列达到一定的长度或者解码出来的token是<eos>时，停止解码。然后以解码出来的序列做为输入，进行下一轮编码解码。

## 3）GAN（Generative Adversarial Networks） 
	模型通过框架中两个模块：生成模型（Generative Model）和判别模型(Discriminative Model)的相互博弈产生预期的输出，原始的GAN理论中，G和D并不一定都必须是神经网络，只要能提供生成和判别的模型即可。
	生成模型使用1）LSTM生成模型即可判别模型使用一个分类器将生成的文章按好坏分为4-6个等级，并根据等级定义判别模型损失，整体GAN模型的损失为生成模型和判别模型的损失之和，所以在理想状态下，生成模型时可以生成出按照判别标准定义为好的文章


​	
## 总结

	使用分词工具jieba或者pkuseg对语料库进行分词，然后以上下文的方式将其切割成上下联的序列，通过词向量（Word Embedding)捕捉词的语义信息，通过编码解码器，得到文本




